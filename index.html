<!doctype html>
<html lang="en">

   
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ“</text></svg>">

  <head>
	<meta name="generator" content="Hugo 0.113.0">
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alegreya:wght@500&family=Alegreya+SC&family=Lora:wght@400&display=swap" rel="stylesheet">

    
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href="/css/style.css">
  </head>

  <body>

    <div class="container">
        <div class="row">
          <div class="col-xl-1"></div>
          <div class="col col-xl-10">
            <div class="row mb-3">
  <div class="col d-none d-md-block">
    <div class="d-flex justify-content-end flex-wrap">
    <a href="https://akshayparuchuri.com/" class="btn link-button" role="button">About</a>
    <a href="https://akshayparuchuri.com/#research" class="btn link-button" role="button">Research</a>
    <a href="https://akshayparuchuri.com/#software" class="btn link-button" role="button">Software</a>
    <a href="/media/Akshay_Paruchuri_CV_Last_Updated_June_2025.pdf" target="_blank" class="btn link-button" role="button">CV</a>
    </div>
  </div>
  <div class="col d-md-none">
    <div class="d-flex justify-content-end flex-wrap">
    <a href="https://akshayparuchuri.com/" class="btn link-button-small" role="button">About</a>
    <a href="https://akshayparuchuri.com/#research" class="btn link-button-small" role="button">Research</a>
    <a href="https://akshayparuchuri.com/#software" class="btn link-button-small" role="button">Software</a>
    <a href="/media/Akshay_Paruchuri_CV_Last_Updated_June_2025.pdf" target="_blank" class="btn link-button-small" role="button">CV</a>
    </div>
  </div>
</div> 


            <div class="row pt-1 m-1">
              
<div class="container" text-align="center">

  <div class="row info-text">
    <div class="col-sm-4">
      <div class="text-center">

        <div class="d-block d-sm-none">
        <img src="headshot.jpg" class="img-fluid rounded-circle center-block about-headshot-mobile" alt="headshot">
        </div>
        <div class="d-none d-sm-block">
        <img src="headshot.jpg" class="img-fluid rounded-circle center-block about-headshot" alt="headshot">
        </div>
        <p class="h3 pt-sm-3 title-name">Akshay Paruchuri</p>

        <div class="d-block d-lg-none">
        <p class="mb-1 title-email-small"><a href="mailto:akshay@cs.unc.edu">akshay@cs.unc.edu</a></p>
        </div>
        <div class="d-none d-lg-block">
        <p class="mb-1 title-email"><a href="mailto:akshay@cs.unc.edu">akshay@cs.unc.edu</a></p>
        </div>

        <a href="https://github.com/yahskapar" class="fa fa-github"></a>
        <a href="https://scholar.google.com/citations?hl=en&user=tc0M7WwAAAAJ&view_op=list_works&sortby=pubdate" class="fa fa-graduation-cap"></a>
        <a href="https://www.linkedin.com/in/akshayparuchuri/" class="fa fa-linkedin"></a>
        <a href="https://twitter.com/Yahskapar" class="fa fa-twitter"></a>
        <div class="d-block d-sm-none">
          <br>
        </div>

      </div>
    </div>
    <div class="col-sm-8 about-text">
      <p>I'm a CS PhD student at <a href="https://cs.unc.edu/">UNC Chapel Hill</a> advised by Professor <a href="https://telepresence.web.unc.edu/">Henry Fuchs</a>. My research interests are at the intersection of computer graphics, computer vision, and machine learning. I work on research that typically involves applications in healthcare and/or augmented reality. Currently, I'm working toward a future where wearable, spatial computing devices, such as augmented reality eyeglasses capable of all-day use, are contextually aware and personalized to the benefit of users and their goals (e.g., human memory enhancement, becoming healthier).</p>

      <p>Currently, I'm a visiting researcher at <a href="https://www.idsia.usi-supsi.ch/">IDSIA USI-SUPSI</a> working with Professor <a href="https://www.pdf.inf.usi.ch/people/piotr/">Piotr Didyk</a>. I've previously done internships at <a href="https://arvr.google.com/">Google AR/VR</a>, <a href="https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/">Google Consumer Health Research</a>, and <a href="https://www.kitware.com/">Kitware</a>. Prior to pursuing a PhD, I was an embedded systems engineer working on wearable devices at <a href="https://www.nike.com/a/nike-adapt-bb-release-info">Nike</a>. I enjoy reading, running, and hiking in my spare time.
      </p>

      <!-- <p>
      I'm happy to chat about research, new opportunities, and life in general with just about anyone. Feel free to reach out via email!
      </p> -->

      <!-- <p>
        <strong>I'm actively looking for Summer 2025 internships in graphics, AR/VR, and application areas such as health and entertainment. Please reach out via email if you have an opportunity that that might be a strong fit!</strong>
      </p> -->

      <span class="news-title">News</span> 
      <div class="expandable-list">
        <ul>

            <li><span class="news-date">Jun 2025</span> <span class="news-entry">Started as a visiting researcher at <a href="https://www.idsia.usi-supsi.ch/">IDSIA USI-SUPSI</a> working with Professor <a href="https://www.pdf.inf.usi.ch/people/piotr/">Piotr Didyk</a></span> </li>

            <li><span class="news-date">Feb 2025</span> <span class="news-entry">Started an internship in the <a href="https://arvr.google.com/">Google AR/VR</a> team with <a href="https://research.google/people/ishanchatterjee/?&type=google">Ishan Chatterjee</a></span> </li>

            <li><span class="news-date">Jan 2025</span> <span class="news-entry">Passed my dissertation proposal and oral exam</a></span> </li>

            <li><span class="news-date">Sep 2024</span> <span class="news-entry"><a href="https://arxiv.org/abs/2406.12830">One first-author paper</a> has been accepted to EMNLP 2024</span> </li>
            
            <li><span class="news-date">Jul 2024</span> <span class="news-entry"><a href="https://ppsnet.github.io/">One first-author paper</a> has been accepted to ECCV 2024</span> </li>
            
            <li><span class="news-date">Mar 2024</span> <span class="news-entry">Started an internship in the <a href="https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/">Google Consumer Health Research</a> team with <a href="https://xliucs.github.io/">Xin Liu</a></span> </li>
          
        </ul>
      
        <span class="news-expand-button news-entry" id="newsExpandBtn">...show all...</span>

      </div>

  
    </div>
  </div>

  <br>
  
  <a name="research"></a> 
  <div class="row content-header">
    <div class="col pr-0">
    <p>Research</p>
    </div>
  </div>

<br>
<p>Please see my <a href="https://scholar.google.com/citations?hl=en&user=tc0M7WwAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> for a full, more up-to-date list of my publications.</p>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/radar/radar_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">RADAR: Benchmarking Language Models on Imperfect Tabular Data</p>
    <p class="summary-authors">Ken Gu, Zhihan Zhang, Kate Lin, Yuwei Zhang, Akshay Paruchuri, Hong Yu, Mehran Kazemi, Kumar Ayush, A. Ali Heydari, Maxwell A. Xu, Yun Liu, Ming-Zher Poh, Yuzhe Yang, Mark Malhotra, Shwetak Patel, Hamid Palangi, Xuhai Xu, Daniel McDuff, Tim Althoff, Xin Liu</p>
    <p class="summary-publication-status"> 
      arXiv 2025
    </p>
    
    <p class="summary-text">To address language models' poor handling of data artifacts, the RADAR benchmark was created to evaluate data awareness on tabular data. Using 2,980 table-query pairs grounded in real-world data spanning 9 domains and 5 data artifact types, RADAR finds that model performance drops significantly when artifacts like missing values are introduced. This reveals a critical gap in their ability to perform robust, real-world data analysis.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://arxiv.org/abs/2506.08249" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="https://huggingface.co/datasets/kenqgu/RADAR" class="btn-sm badge-button" role="button">dataset</a>
      
      </div>

      <div>
      
        <a href="https://github.com/kenqgu/RADAR" class="btn-sm badge-button" role="button">code</a>
      
      </div>

      <div>
      
        <a href="/media/papers/radar/radar_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">
  
  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/prob_reasoning_in_LLMs/prob_reasoning_in_LLMs.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">What Are the Odds? Language Models Are Capable of Probabilistic Reasoning</p>
    <p class="summary-authors">Akshay Paruchuri, Jake Garrison, Shun Liao, John Hernandez, Jacob Sunshine, Tim Althoff, Xin Liu, Daniel McDuff</p>
    <p class="summary-publication-status"> 
      EMNLP 2024 (Main)
    </p>
    
    <p class="summary-text">Language models were evaluated on probabilistic reasoning tasks such as estimating percentiles and calculating probabilities using idealized and real-world distributions. Techniques including within-distribution anchoring and simplifying assumptions significantly improved LLM performance by up to 70%.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://arxiv.org/pdf/2406.12830" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="https://github.com/yahskapar/LLMs-and-Probabilistic-Reasoning" class="btn-sm badge-button" role="button">code</a>
      
      </div>

      <div>
      
        <a href="https://research.google/blog/evaluating-and-enhancing-probabilistic-reasoning-in-language-models/" class="btn-sm badge-button" role="button">blog post</a>
      
      </div>

      <div>
      
        <a href="/media/papers/prob_reasoning_in_LLMs/prob_reasoning_in_LLMs_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/phia/phia_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Transforming Wearable Data into Health Insights using Large Language Model Agents</p>
    <p class="summary-authors">Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu</p>
    <p class="summary-publication-status"> 
      arXiv 2024
    </p>
    
    <p class="summary-text">The Personal Health Insights Agent (PHIA) leverages large language models with code generation and information retrieval tools to enable personalized insights from wearable health data, an ongoing challenge. Evaluated on over 4000 questions, PHIA accurately answers over 84% of factual numerical and 83% of open-ended health questions, paving the way for accessible, data-driven personalized wellness.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://arxiv.org/abs/2406.06464" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="/media/papers/phia/phia_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/colon_image_translation/colon_image_translation_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Structure-preserving Image Translation for Depth Estimation in Colonoscopy Video</p>
    <p class="summary-authors">Shuxian Wang, Akshay Paruchuri, Zhaoxi Zhang, Sarah McGill, Roni Sengupta</p>
    <p class="summary-publication-status"> 
      MICCAI 2024 (Oral)
    </p>
    
    <p class="summary-text">To address the domain gap in colonoscopy depth estimation, a structure-preserving synthetic-to-real image translation pipeline generates realistic synthetic images that retain depth geometry. This approach, aided by a new clinical dataset, improves supervised depth estimation and generalization to real-world clinical data.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://arxiv.org/abs/2408.10153" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="/media/papers/colon_image_translation/colon_image_translation_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/ppsnet/ppsnet_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos</p>
    <p class="summary-authors">Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta</p>
    <p class="summary-publication-status"> 
      ECCV 2024
    </p>
    
    <p class="summary-text">Near-field lighting in endoscopes is modeled as Per-Pixel Shading (PPS) to achieve state-of-the-art depth refinement on colonoscopy data. This is accomplished using PPS features with teacher-student transfer learning and PPS-informed self-supervision.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://ppsnet.github.io/" class="btn-sm badge-button" role="button">project</a>
      
      </div>
    
      <div>
      
        <a href="https://arxiv.org/abs/2403.17915" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="https://github.com/yahskapar/PPSNet" class="btn-sm badge-button" role="button">code</a>
      
      </div>

      <div>
      
        <a href="/media/papers/ppsnet/ppsnet_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/marppg/marppg_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement</p>
    <p class="summary-authors">Akshay Paruchuri, Xin Liu, Yulu Pan, Shwetak Patel, Daniel McDuff, Roni Sengupta</p>
    <p class="summary-publication-status"> 
      WACV 2024 (Oral)
    </p>
    
    <p class="summary-text">Neural Motion Transfer is presented as an effective data augmentation technique for estimating PPG signals from facial videos. This approach significantly improved inter-dataset testing results by up to 79% and outperformed existing state-of-the-art methods on the PURE dataset by 47%.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="https://motion-matters.github.io/" class="btn-sm badge-button" role="button">project</a>
      
      </div>
    
      <div>
      
        <a href="https://arxiv.org/abs/2303.12059" class="btn-sm badge-button" role="button">arxiv</a>
      
      </div>

      <div>
      
        <a href="https://github.com/yahskapar/MA-rPPG-Video-Toolbox" class="btn-sm badge-button" role="button">code</a>
      
      </div>

      <div>
      
        <a href="/media/papers/marppg/marppg_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/ego_collab_recon/ego_collab_recon_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Reconstruction of Human Body Pose and Appearance Using Body-Worn IMUs and a Nearby Camera View for Collaborative Egocentric Telepresence</p>
    <p class="summary-authors">Qian Zhang, Akshay Paruchuri, Young-Woon Cha, Jia-Bin Huang, Jade Kandel, Howard Jiang, Adrian Ilie, Andrei State, Danielle Szafir, Daniel Szafir, Henry Fuchs</p>
    <p class="summary-publication-status"> 
      IEEE VR 2023 (ReDigiTS Workshop)
    </p>
    
    <p class="summary-text">A collaborative 3D reconstruction method estimates a target person's body pose using worn IMUs and reconstructs their appearance via an external AR headset view from another nearby person. This approach aims to enable future anytime, anywhere telepresence through daily worn accessories.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="/media/papers/ego_collab_recon/zhang2023reconstruction.pdf"  target="_blank" class="btn-sm badge-button" role="button">PDF</a>
      
      </div>

      <div>
      
        <a href="/media/papers/ego_collab_recon/ego_collab_recon_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=/media/papers/drone_brush/drone_brush_teaser.jpg class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">Drone Brush: Mixed Reality Drone Path Planning</p>
    <p class="summary-authors">Angelos Angelopoulos, Austin Hale, Husam Shaik, Akshay Paruchuri, Ken Liu, Randal Tuggle, Daniel Szafir</p>
    <p class="summary-publication-status"> 
      HRI 2022
    </p>
    
    <p class="summary-text">Drone Brush introduces a mixed reality interface using HoloLens 2 for intuitive 3D drone path planning with hand gestures, featuring collision checking via spatial maps and path simplification.</p>
    
    <div class="d-flex flex-row flex-wrap">
    
      <div>
      
        <a href="/media/papers/drone_brush/2022angelopoulosdrone.pdf"  target="_blank" class="btn-sm badge-button" role="button">PDF</a>
      
      </div>

      <div>
      
        <a href="/media/papers/drone_brush/drone_brush_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>
    
    </div>
  </div>

</div>

  <br>
  <br>

  <a name="software"></a> 
  <div class="row content-header">
    <div class="col pr-0">
    <p>Software</p>
    </div>
  </div>
  
    
<div class="row content-summary pt-4 pb-2">

  <div class="d-none d-sm-block col-sm-3  m-0 p-0">
    
    <img src=media/software/rppg-toolbox/rppg_toolbox_teaser.png class="img-fluid summary-image drop-shadow" alt="teaser img">
    
  </div>
  
  <div class="col-xs col-sm-9">
    <p class="summary-title">rPPG-Toolbox: Deep Remote PPG Toolbox</p>
    <p class="summary-authors"></p>
    <p class="summary-publication-status"> 
      NeurIPS 2023 Datasets and Benchmarks Track
      
    </p>
    
    <p class="summary-text">A comprehensive toolbox that contains unsupervised and supervised remote photoplethysmography (rPPG) models with support for public benchmark datasets, data augmentation, and systematic evaluation.</p>
    
    <div class="d-flex flex-row flex-wrap">

      <div>
      
        <a href="https://arxiv.org/abs/2210.00716" class="btn-sm badge-button" role="button">arXiv</a>
      
      </div>

      <div>
      
        <a href="https://github.com/ubicomplab/rPPG-Toolbox" class="btn-sm badge-button" role="button">code</a>
      
      </div>

      <div>
      
        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/d7d0d548a6317407e02230f15ce75817-Abstract-Datasets_and_Benchmarks.html" class="btn-sm badge-button" role="button">NeurIPS 2023</a>
      
      </div>

      <div>
      
        <a href="/media/papers/texfusion/texfusion_bib.txt"  target="_blank" class="btn-sm badge-button" role="button">bibtex</a>
      
      </div>

    </div>
  </div>

</div>


<script>
  document.addEventListener("DOMContentLoaded", function() {
    const newsExpandBtn = document.getElementById("newsExpandBtn");
    const extraItems = document.querySelectorAll(".extra-item");
    let isExpanded = false;

    newsExpandBtn.addEventListener("click", function() {
      if (isExpanded) {
        extraItems.forEach(item => item.style.display = "none");
      } else {
        extraItems.forEach(item => item.style.display = "list-item");
      }
      isExpanded = !isExpanded;
    });
  });
</script>


            </div>
          </div>
          <div class="col-xl-1"></div>
        </div>
    </div>

    
    
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
  </body>
    
  <br>
  <br>
  <br>
  <br>

  <footer>
    <div class="row">
      <div class="col-12">
        <p class="text-center">Website design originally by <a href="https://nmwsharp.com/">Nicholas Sharp</a>. Check out his work!</p>
        <br>
        <br>
        <br>
        <br>
      </div>
    </div>
  </footer>


</html>
